{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: House Prices - Advanced Regression Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the Ames housing dataset\n",
    "train = pd.read_csv('data/house_prices/train.csv')\n",
    "test = pd.read_csv('data/house_prices/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ames housing train dataset has 1460 data points with 81 variables each.\n",
      "Ames housing test dataset has 1459 data points with 80 variables each.\n"
     ]
    }
   ],
   "source": [
    "# Size of the dataset\n",
    "print \"Ames housing train dataset has {} data points with {} variables each.\".format(*train.shape)\n",
    "print \"Ames housing test dataset has {} data points with {} variables each.\".format(*test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First approach: without feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removal of Null values\n",
    "train_fillna = train.fillna(value = 0.0)\n",
    "test_fillna = test.fillna(value = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Top 2 most corralated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# splitting the train set\n",
    "features = train_fillna[['OverallQual', 'GrLivArea']]\n",
    "prices = train_fillna['SalePrice']\n",
    "log_prices = np.log1p(prices)\n",
    "\n",
    "# splitting the test set\n",
    "public_features = test_fillna[['OverallQual', 'GrLivArea']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1456, 2)\n",
      "(1459, 2)\n"
     ]
    }
   ],
   "source": [
    "print features.shape\n",
    "print public_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the dataset with selected features\n",
    "# the easiest way is to pickle it using to_pickle:\n",
    "\n",
    "features.to_pickle('features_top2.pkl')  # where to save it, usually as a .pkl\n",
    "log_prices.to_pickle('log_prices_top2.pkl')\n",
    "\n",
    "public_features.to_pickle('public_features_top2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Top ten most corralated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# splitting the train set\n",
    "# selected features or all continuous features\n",
    "features = train_fillna[['OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF','1stFlrSF','FullBath','TotRmsAbvGrd','YearBuilt','YearRemodAdd']]\n",
    "prices = train_fillna['SalePrice']\n",
    "log_prices = np.log1p(prices)\n",
    "\n",
    "# splitting the test set\n",
    "public_features = test_fillna[['OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF','1stFlrSF','FullBath','TotRmsAbvGrd','YearBuilt','YearRemodAdd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the dataset with selected features\n",
    "# the easiest way is to pickle it using to_pickle:\n",
    "\n",
    "features.to_pickle('features_top10.pkl')  # where to save it, usually as a .pkl\n",
    "log_prices.to_pickle('log_prices_top10.pkl')\n",
    "\n",
    "public_features.to_pickle('public_features_top10.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Working with all the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All missing values will be replaced either by o in the case of the numeric features or by the mean in the case of the categorical features\n",
    "\n",
    "* LabelEncoder will be applied to the Ordinal features.\n",
    "\n",
    "* One-Hot-Encoding will be applied to the Nominal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ames housing train dataset has 1456 data points with 81 variables each.\n"
     ]
    }
   ],
   "source": [
    "# There are 4 houses with more than 4000 sq ft living area that are\n",
    "# outliers, so we drop them from the training data.\n",
    "train.drop(train[train['GrLivArea'] > 4000].index, inplace=True)\n",
    "print 'Ames housing train dataset has {} data points with {} variables each.'.format(*train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with mean of column.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "\n",
    "def label_encoding(X):\n",
    "    ''' Preprocesses the dataset data and converts ordinal variables into labels with value between 0 and n_classes-1. '''\n",
    "    \n",
    "    # retain all columns LabelEncoder as dictionary\n",
    "    d = defaultdict(LabelEncoder)\n",
    "\n",
    "    # Encoding the variable\n",
    "    fit = X.apply(lambda x: d[x.name].fit_transform(x))\n",
    "\n",
    "    # Inverse the encoded\n",
    "    fit.apply(lambda x: d[x.name].inverse_transform(x))\n",
    "\n",
    "    # Using the dictionary to label future data\n",
    "    # Example: 'BsmtCond' => 'NA' = - 'Po' = 3 'Fa' = 1 'TA' = 4 'Gd' = 2 and 'Ex' = -\n",
    "    output = X.apply(lambda x: d[x.name].transform(x))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BsmtCond BsmtExposure\n",
      "0       TA           No\n",
      "1       TA           Gd\n",
      "2       TA           Mn\n",
      "4       TA           Av\n",
      "5       TA           No\n"
     ]
    }
   ],
   "source": [
    "# debugging\n",
    "x = train[['BsmtCond','BsmtExposure']]\n",
    "print x[x['BsmtCond'] == 'TA'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BsmtCond        3\n",
      "BsmtExposure    2\n",
      "Name: 375, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# debugging\n",
    "a = train[['BsmtCond','BsmtExposure']]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "d = defaultdict(LabelEncoder)\n",
    "\n",
    "# Encoding the variable\n",
    "fit = a.apply(lambda x: d[x.name].fit_transform(x))\n",
    "\n",
    "# Inverse the encoded\n",
    "fit.apply(lambda x: d[x.name].inverse_transform(x))\n",
    "\n",
    "# Using the dictionary to label future data\n",
    "at = a.apply(lambda x: d[x.name].transform(x))\n",
    "print at.loc[375]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature columns (2 total features):\n",
      "['BsmtExposure', 'Alley']\n",
      "   BsmtExposure  Alley\n",
      "0             4      0\n",
      "1             2      0\n",
      "2             3      0\n",
      "3             4      0\n",
      "4             1      0\n"
     ]
    }
   ],
   "source": [
    "# debugging\n",
    "a = train[['BsmtExposure','Alley']]\n",
    "at = label_encoding(a)\n",
    "print \"Processed feature columns ({} total features):\\n{}\".format(len(at.columns), list(at.columns))\n",
    "\n",
    "# Show the feature information by printing the first five rows\n",
    "print at.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(X):\n",
    "    ''' Preprocesses the dataset data and converts nominal variables into dummy variables. '''\n",
    "    \n",
    "    # Initialize new output DataFrame\n",
    "    output = pd.DataFrame(index = X.index)\n",
    "\n",
    "    # Investigate each feature column for the data\n",
    "    for col, col_data in X.iteritems():\n",
    "\n",
    "        # Categorical data to dummy variables  \n",
    "        # Example: 'Alley' => 'Alley_Grvl' and 'Alley_Pave'\n",
    "        col_data = pd.get_dummies(col_data, prefix = col)  \n",
    "        \n",
    "        # Collect the revised columns\n",
    "        output = output.join(col_data)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BsmtExposure Alley\n",
      "0           No   NaN\n",
      "1           Gd   NaN\n",
      "2           Mn   NaN\n",
      "3           No   NaN\n",
      "4           Av   NaN\n"
     ]
    }
   ],
   "source": [
    "# debugging\n",
    "b = train[['BsmtExposure','Alley']]\n",
    "print b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   BsmtExposure_Av  BsmtExposure_Gd  BsmtExposure_Mn  BsmtExposure_No  \\\n",
      "0              0.0              0.0              0.0              1.0   \n",
      "1              0.0              1.0              0.0              0.0   \n",
      "2              0.0              0.0              1.0              0.0   \n",
      "3              0.0              0.0              0.0              1.0   \n",
      "4              1.0              0.0              0.0              0.0   \n",
      "\n",
      "   Alley_Grvl  Alley_Pave  \n",
      "0         0.0         0.0  \n",
      "1         0.0         0.0  \n",
      "2         0.0         0.0  \n",
      "3         0.0         0.0  \n",
      "4         0.0         0.0  \n"
     ]
    }
   ],
   "source": [
    "# debugging\n",
    "bt = one_hot(b)\n",
    "print bt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_skewed(X):\n",
    "    ''' Transform the skewed numeric features. '''\n",
    "    # Initialize new output DataFrame\n",
    "    output = pd.DataFrame(index = X.index)\n",
    "\n",
    "    # Transform the skewed numeric features by taking log1p.\n",
    "    from scipy.stats import skew\n",
    "\n",
    "    skewed = X.apply(lambda x: skew(x.dropna().astype(float)))\n",
    "    skewed = skewed[skewed > 0.75]\n",
    "    skewed = skewed.index\n",
    "    output = np.log1p(X[skewed])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standard_escaling(X):\n",
    "    ''' Preprocesses the dataset data and converts ordinal variables into labels with value between 0 and n_classes-1. '''\n",
    "    \n",
    "    # retain all columns LabelEncoder as dictionary\n",
    "    d = defaultdict(StandardScaler)\n",
    "\n",
    "    # Encoding the variable\n",
    "    fit = X.apply(lambda x: d[x.name].fit_transform(x))\n",
    "\n",
    "    # Inverse the encoded\n",
    "    fit.apply(lambda x: d[x.name].inverse_transform(x))\n",
    "\n",
    "    # Using the dictionary to label future data\n",
    "    # Example: 'BsmtCond' => 'NA' = - 'Po' = 3 'Fa' = 1 'TA' = 4 'Gd' = 2 and 'Ex' = -\n",
    "    output = X.apply(lambda x: d[x.name].transform(x))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering(dataset):\n",
    "    ''' Transforms all the features and output a new data frame of engineered features. '''\n",
    "    \n",
    "    output = pd.DataFrame(index = dataset.index)\n",
    "    \n",
    "    # Removal of null values\n",
    "    dataset_filled = DataFrameImputer().fit_transform(dataset)\n",
    "    \n",
    "    # Numeric features\n",
    "    numeric_features = dataset_filled[['1stFlrSF', '2ndFlrSF', '3SsnPorch', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'EnclosedPorch', 'GarageArea', 'GrLivArea', 'LotArea', 'LotFrontage', 'LowQualFinSF', 'MasVnrArea', 'MiscVal', 'OpenPorchSF', 'PoolArea', 'ScreenPorch', 'TotalBsmtSF', 'WoodDeckSF', 'BedroomAbvGr', 'BsmtFullBath', 'BsmtHalfBath', 'Fireplaces', 'FullBath', 'GarageCars', 'GarageYrBlt', 'HalfBath', 'KitchenAbvGr', 'MoSold', 'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd', 'YrSold']]\n",
    "    output = output.join(numeric_features)\n",
    "    \n",
    "    ## Transform Nominal features\n",
    "    # All nominal features\n",
    "    nominal_features = dataset_filled[['Alley', 'BldgType', 'CentralAir', 'Condition1', 'Condition2', 'Exterior1st', 'Exterior2nd', 'Foundation', 'GarageType', 'Heating', 'HouseStyle', 'LandContour', 'LotConfig', 'MasVnrType', 'MiscFeature', 'MSSubClass', 'MSZoning', 'Neighborhood', 'RoofMatl', 'RoofStyle', 'SaleCondition', 'SaleType', 'Street']]\n",
    "    # Collect the revised columns\n",
    "    engineered_nominal_features = label_encoding(nominal_features)\n",
    "    output = output.join(engineered_nominal_features)\n",
    "    \n",
    "    ## Transform Ordinal features\n",
    "    # All ordinal features\n",
    "    ordinal_features = dataset_filled[['BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual', 'Electrical', 'ExterCond', 'ExterQual', 'Fence', 'FireplaceQu', 'Functional', 'GarageCond', 'GarageFinish', 'GarageQual', 'HeatingQC', 'KitchenQual', 'LandSlope', 'LotShape', 'OverallCond', 'OverallQual', 'PavedDrive', 'PoolQC', 'Utilities']]\n",
    "    \n",
    "    # Collect the revised columns\n",
    "    engineered_ordinal_features = one_hot(ordinal_features)\n",
    "    output = output.join(engineered_ordinal_features)\n",
    "    \n",
    "    ## Skewed (this actually lower the score!)\n",
    "    # skewed_features = transform_skewed(numeric_features)\n",
    "    # output = output.join(skewed_features)\n",
    "    \n",
    "    ## Scale the data (this also lower the score!)\n",
    "    # scaled_features = standard_escaling(numeric_features)\n",
    "    # output = output.join(scaled_features)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature engineering to datasets\n",
    "train_engineered = feature_engineering(train)\n",
    "test_engineered = feature_engineering(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unique columns if the train and test set have different shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# check if the train and test set have the same shape\n",
    "if train_engineered.shape[1] > test_engineered.shape[1]:\n",
    "    features_to_drop = []\n",
    "    for feature in train_engineered.columns:\n",
    "        if feature not in test_engineered.columns:\n",
    "            features_to_drop.append(feature)\n",
    "    train_engineered.drop(features_to_drop, axis=1, inplace=True)\n",
    "    print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if train_engineered.shape[1] < test_engineered.shape[1]:\n",
    "    features_to_drop = []\n",
    "    for feature in test_engineered.columns:\n",
    "        if feature not in train_engineered:\n",
    "            features_to_drop.append(feature)\n",
    "    test_engineered.drop(features_to_drop, axis=1, inplace=True)\n",
    "    print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# retrieve price information\n",
    "log_prices = pd.DataFrame(index = train_engineered.index, columns=[\"SalePrice\"])\n",
    "log_prices[\"SalePrice\"] = np.log(train[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the datasets using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the dataset with selected features\n",
    "# the easiest way is to pickle it using to_pickle:\n",
    "train_engineered.to_pickle('features_all.pkl')  # where to save it, usually as a .pkl\n",
    "log_prices.to_pickle('log_prices_all.pkl')\n",
    "\n",
    "test_engineered.to_pickle('public_features_all.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting correlation of engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "train_engineered['SalePrice'] = log_prices\n",
    "corr = train_engineered.corr()['SalePrice'].sort_values(ascending = False)\n",
    "\n",
    "corr.to_csv('correlation.csv', header=True, index_label='Id') # add 1 to csv name every time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
