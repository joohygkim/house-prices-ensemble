{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: House Prices - Advanced Regression Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import pickle\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the corresponding dataset via pickle\n",
    "\n",
    "Run only one of the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading datasets via pickle:\n",
    "# Work with the top two most corralated features\n",
    "features = pd.read_pickle('features_top2.pkl')\n",
    "log_prices = pd.read_pickle('log_prices_top2.pkl')\n",
    "public_features = pd.read_pickle('public_features_top2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading datasets via pickle:\n",
    "# Work with the top ten most corralated features\n",
    "features = pd.read_pickle('features_top10.pkl')\n",
    "log_prices = pd.read_pickle('log_prices_top10.pkl')\n",
    "public_features = pd.read_pickle('public_features_top10.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading datasets via pickle:\n",
    "# Work with all features\n",
    "features = pd.read_pickle('features_all.pkl')\n",
    "log_prices = pd.read_pickle('log_prices_all.pkl')\n",
    "public_features = pd.read_pickle('public_features_all.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import 'train_test_split'\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Shuffle and split the data into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, log_prices, test_size=0.2, random_state=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch to all regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining hyperparameters for all regressors to be tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DecisionTreeRegressor: \n",
    "splitter = ['best', 'random']\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "max_depth_range = np.arange(1, 11)\n",
    "min_samples_split = np.arange(2, 9)\n",
    "min_samples_leaf = np.arange(1, 9)\n",
    "param_grid_A = dict(splitter=splitter, max_features=max_features, max_depth=max_depth_range, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVR: \n",
    "C = [0.9, 1.0, 1.1]\n",
    "kernel = ['linear', 'rbf']\n",
    "max_iter = [1500]\n",
    "param_grid_B = dict(C=C, kernel=kernel, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ElasticNet: \n",
    "alpha = [1.0, 1.1]\n",
    "tol = [0.00005, 0.0001, 0.00015, 0.0002]\n",
    "param_grid_C = dict(alpha=alpha, tol=tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lasso: \n",
    "alpha = [0.01, 1, 10]\n",
    "tol = [0.00005, 0.0001, 0.00015, 0.0002]\n",
    "selection = ['cyclic', 'random']\n",
    "param_grid_D = dict(alpha=alpha, tol=tol, selection=selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LassoLars: \n",
    "alpha = [0.0001, 0.001, 0.01, 1]\n",
    "normalize = ['True', 'False']\n",
    "param_grid_E = dict(alpha=alpha, normalize=normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BayesianRidge: \n",
    "tol = [0.0001, 0.001, 0.01, 0.1,  1, 10, 50, 100, 500]\n",
    "n_iter= [1000]\n",
    "normalize = ['True', 'False']\n",
    "# values = np.arange(0.5, 8) / 1000000.0\n",
    "# alpha_1 = values\n",
    "# alpha_2 = values\n",
    "# lambda_1 = values\n",
    "# lambda_2 = values\n",
    "param_grid_F = dict(n_iter=n_iter, tol=tol, normalize=normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GradientBoostingRegressor:\n",
    "# alpha = [0.9, 1]\n",
    "# learning_rate = [0.01, 0.1, 1]\n",
    "# n_estimators = np.arange(100, 200, 50)\n",
    "# loss = ['ls', 'lad']\n",
    "max_depth_range = np.arange(1, 6)\n",
    "criterion = ['friedman_mse', 'mse']\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "min_samples_split = np.arange(2, 6)\n",
    "min_samples_leaf = np.arange(1, 10)\n",
    "param_grid_G = dict(max_depth=max_depth_range, criterion=criterion, max_features=max_features, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ExtraTreesRegressor: \n",
    "# n_estimators = np.arange(50, 500, 50)\n",
    "# max_features = ['auto', 'sqrt', 'log2']\n",
    "# max_depth_range = np.arange(1, 6)\n",
    "min_samples_split = np.arange(2, 6)\n",
    "min_samples_leaf = np.arange(1, 6)\n",
    "param_grid_H = dict(min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BaggingRegressor: \n",
    "n_estimators = np.arange(50, 500, 50)\n",
    "# max_features = [1.0, 2.0]\n",
    "# max_samples = np.arange(0.0, 2.0)\n",
    "param_grid_I = dict(n_estimators=n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AdaBoostRegressor: \n",
    "learning_rate = [0.001, 0.01, 1.0]\n",
    "# n_estimators = np.arange(50, 500, 50)\n",
    "loss = ['linear', 'square', 'exponential']\n",
    "param_grid_J = dict(learning_rate=learning_rate, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# XGBRegressor:\n",
    "colsample_bytree = [0.9, 1.0]\n",
    "gamma = [0.0, 0.1]\n",
    "learning_rate = [0.1, 0.5]\n",
    "max_depth = np.arange(3, 5)\n",
    "min_child_weight = np.arange(1, 4)\n",
    "n_estimators = np.linspace(100, 10000, 100)\n",
    "reg_alpha =  [0.0, 0.05, 0.1, 0.2, 0.3, 0.4]\n",
    "reg_lambda = [0.6, 0.7, 0.8, 0.9, 0.95, 1.0]\n",
    "subsample = [0.6, 0.7, 0.8, 0.9, 0.95, 1.0]\n",
    "\n",
    "param_grid_K = dict(colsample_bytree =colsample_bytree, gamma=gamma, max_depth=max_depth, reg_alpha=reg_alpha, reg_lambda=reg_lambda,  subsample=subsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the models dictionary file via pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the regressors dictioanry\n",
    "filename = 'regs_dict.dict'\n",
    "\n",
    "# load the models dictionary according to selected filename\n",
    "regs_dict = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# top2 features with untuned regressors\n",
    "# selected_regressors = regs_dict['top_2']['untuned']\n",
    "\n",
    "# top10 features with untuned regressors\n",
    "# selected_regressors = regs_dict['top_10']['untuned']\n",
    "\n",
    "# all features with untuned regressors\n",
    "selected_regressors = regs_dict['all']['untuned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine regressors and hyperparameters in a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tune all models\n",
    "models = ['DecisionTreeRegressor', 'SVR', 'ElasticNet', 'Lasso', 'LassoLars', 'BayesianRidge', 'GradientBoostingRegressor', 'ExtraTreesRegressor', 'BaggingRegressor', 'AdaBoostRegressor', 'XGBRegressor']\n",
    "param_grids = [param_grid_A, param_grid_B, param_grid_C, param_grid_D, param_grid_E, param_grid_F, param_grid_G, param_grid_H, param_grid_I, param_grid_J, param_grid_K]\n",
    "regressor_tuples = zip(models, param_grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# debugging one by one\n",
    "models = ['SVR']\n",
    "param_grids = [param_grid_B]\n",
    "regressor_tuples = zip(models, param_grids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tune_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from time import time\n",
    "\n",
    "# initialize empty dictionary\n",
    "tuned_regressors = {}\n",
    "\n",
    "# loop throught all regressors and perform gridsearch\n",
    "def tune_parameters(regressor_tuples):\n",
    "    for reg, param_grid in regressor_tuples:\n",
    "        # Regressor name\n",
    "        print '{}\\n'.format(selected_regressors[reg].__class__.__name__)\n",
    "        \n",
    "        # Indicate the classifier and the training set size\n",
    "        print \"Training a {} using a training set size of {}. . .\".format(selected_regressors[reg].__class__.__name__, len(X_train))\n",
    "        \n",
    "        # Old regressor\n",
    "        print '{} Old regressor:\\n{}\\n'.format(selected_regressors[reg].__class__.__name__, selected_regressors[reg])\n",
    "        \n",
    "        start = time()\n",
    "        from sklearn.metrics import mean_squared_error as mse\n",
    "        def rmse(y_pred, y_test):\n",
    "            #y_pred = reg.predict(X_test)\n",
    "            mse_score = mse(y_test, y_pred)\n",
    "            rmse_score = np.sqrt(mse_score)\n",
    "            return rmse_score\n",
    "\n",
    "        # Make an rmse scoring function using 'make_scorer' \n",
    "        rmse_scorer = make_scorer(rmse, greater_is_better=False)\n",
    "        \n",
    "        # K-fold\n",
    "        kf = KFold(n_splits=4, random_state=2, shuffle=True)\n",
    "        cv = kf\n",
    "\n",
    "        # Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "        grid_obj = GridSearchCV(selected_regressors[reg], param_grid, scoring=rmse_scorer)\n",
    "\n",
    "        # Fit the grid search object to the training data and find the optimal parameters\n",
    "        start_train = time()\n",
    "        grid_obj = grid_obj.fit(X_train, y_train)\n",
    "        end_train = time()\n",
    "        \n",
    "        # Get the estimator\n",
    "        reg = grid_obj.best_estimator_\n",
    "        tuned_regressors[reg.__class__.__name__] = reg\n",
    "        end_grid = time()\n",
    "        \n",
    "        # tuned regressor\n",
    "        print 'Tuned regressor:\\n{}\\n'.format(reg)\n",
    "\n",
    "        # Calculate rmsle\n",
    "        start_test = time()\n",
    "        # Predict \n",
    "        y_pred = reg.predict(X_test)\n",
    "        rmse_score = rmse(y_pred, y_test)\n",
    "        \n",
    "        end_test = time()\n",
    "        train_time = end_train - start_train\n",
    "        grid_time = end_grid - start\n",
    "        test_time = end_test - start_test\n",
    "        \n",
    "        # calculate r2 score\n",
    "        from sklearn.metrics import r2_score\n",
    "        score = reg.score(X_test, y_test)\n",
    "        \n",
    "        # training time\n",
    "        print 'Trained model in: {}'.format(train_time)\n",
    "        \n",
    "        # testing time\n",
    "        print 'Test model in : {}'.format(test_time)\n",
    "        \n",
    "        # grid search time\n",
    "        print 'GridSearchCV performed in : {}\\n'.format(grid_time)\n",
    "        \n",
    "        # r2 score\n",
    "        print 'r2 score is: {}'.format(score)\n",
    "        \n",
    "        # rmsle score\n",
    "        print 'rmsle score is: {}\\n'.format(rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform grid search of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressor\n",
      "\n",
      "Training a DecisionTreeRegressor using a training set size of 1164. . .\n",
      "DecisionTreeRegressor Old regressor:\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, presort=False, random_state=2,\n",
      "           splitter='best')\n",
      "\n",
      "Tuned regressor:\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=7, max_features='auto',\n",
      "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "           min_samples_leaf=8, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, presort=False, random_state=2,\n",
      "           splitter='best')\n",
      "\n",
      "Trained model in: 259.116488934\n",
      "Test model in : 0.00585913658142\n",
      "GridSearchCV performed in : 259.117077827\n",
      "\n",
      "r2 score is: 0.725685598036\n",
      "rmsle score is: 0.213334028096\n",
      "\n",
      "SVR\n",
      "\n",
      "Training a SVR using a training set size of 1164. . .\n",
      "SVR Old regressor:\n",
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py:220: ConvergenceWarning: Solver terminated early (max_iter=1500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned regressor:\n",
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=1500, shrinking=True, tol=0.001, verbose=False)\n",
      "\n",
      "Trained model in: 7.74384188652\n",
      "Test model in : 0.105340003967\n",
      "GridSearchCV performed in : 7.7450940609\n",
      "\n",
      "r2 score is: -0.0055317210357\n",
      "rmsle score is: 0.408445124859\n",
      "\n",
      "ElasticNet\n",
      "\n",
      "Training a ElasticNet using a training set size of 1164. . .\n",
      "ElasticNet Old regressor:\n",
      "ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
      "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "      random_state=2, selection='cyclic', tol=0.0001, warm_start=False)\n",
      "\n",
      "Tuned regressor:\n",
      "ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
      "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "      random_state=2, selection='cyclic', tol=0.0002, warm_start=False)\n",
      "\n",
      "Trained model in: 3.20535993576\n",
      "Test model in : 0.00382208824158\n",
      "GridSearchCV performed in : 3.20547318459\n",
      "\n",
      "r2 score is: 0.836941029502\n",
      "rmsle score is: 0.164478136519\n",
      "\n",
      "Lasso\n",
      "\n",
      "Training a Lasso using a training set size of 1164. . .\n",
      "Lasso Old regressor:\n",
      "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=2,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned regressor:\n",
      "Lasso(alpha=0.01, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=2,\n",
      "   selection='random', tol=5e-05, warm_start=False)\n",
      "\n",
      "Trained model in: 15.4352319241\n",
      "Test model in : 0.00264286994934\n",
      "GridSearchCV performed in : 15.4354019165\n",
      "\n",
      "r2 score is: 0.840188763331\n",
      "rmsle score is: 0.16283189797\n",
      "\n",
      "LassoLars\n",
      "\n",
      "Training a LassoLars using a training set size of 1164. . .\n",
      "LassoLars Old regressor:\n",
      "LassoLars(alpha=1.0, copy_X=True, eps=2.2204460492503131e-16,\n",
      "     fit_intercept=True, fit_path=True, max_iter=500, normalize=True,\n",
      "     positive=False, precompute='auto', verbose=False)\n",
      "\n",
      "Tuned regressor:\n",
      "LassoLars(alpha=0.0001, copy_X=True, eps=2.2204460492503131e-16,\n",
      "     fit_intercept=True, fit_path=True, max_iter=500, normalize='True',\n",
      "     positive=False, precompute='auto', verbose=False)\n",
      "\n",
      "Trained model in: 1.16700983047\n",
      "Test model in : 0.00208282470703\n",
      "GridSearchCV performed in : 1.16716885567\n",
      "\n",
      "r2 score is: 0.901367882974\n",
      "rmsle score is: 0.127921930953\n",
      "\n",
      "BayesianRidge\n",
      "\n",
      "Training a BayesianRidge using a training set size of 1164. . .\n",
      "BayesianRidge Old regressor:\n",
      "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
      "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
      "       normalize=False, tol=0.001, verbose=False)\n",
      "\n",
      "Tuned regressor:\n",
      "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
      "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=1000,\n",
      "       normalize='True', tol=10, verbose=False)\n",
      "\n",
      "Trained model in: 5.61688804626\n",
      "Test model in : 0.00218796730042\n",
      "GridSearchCV performed in : 5.61704707146\n",
      "\n",
      "r2 score is: 0.903657922958\n",
      "rmsle score is: 0.126428164098\n",
      "\n",
      "GradientBoostingRegressor\n",
      "\n",
      "Training a GradientBoostingRegressor using a training set size of 1164. . .\n",
      "GradientBoostingRegressor Old regressor:\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "             warm_start=False)\n",
      "\n",
      "Tuned regressor:\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=4,\n",
      "             max_features='sqrt', max_leaf_nodes=None,\n",
      "             min_impurity_split=1e-07, min_samples_leaf=5,\n",
      "             min_samples_split=3, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False)\n",
      "\n",
      "Trained model in: 1077.85089898\n",
      "Test model in : 0.00888109207153\n",
      "GridSearchCV performed in : 1077.85107207\n",
      "\n",
      "r2 score is: 0.891550998814\n",
      "rmsle score is: 0.13413700561\n",
      "\n",
      "ExtraTreesRegressor\n",
      "\n",
      "Training a ExtraTreesRegressor using a training set size of 1164. . .\n",
      "ExtraTreesRegressor Old regressor:\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None,\n",
      "          min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=1, oob_score=False, random_state=2,\n",
      "          verbose=0, warm_start=False)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_validation.py:238: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.py:645: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  best_estimator.fit(X, y, **self.fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned regressor:\n",
      "ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
      "          max_features='auto', max_leaf_nodes=None,\n",
      "          min_impurity_split=1e-07, min_samples_leaf=4,\n",
      "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "          n_estimators=10, n_jobs=1, oob_score=False, random_state=2,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "Trained model in: 15.0411059856\n",
      "Test model in : 0.0138528347015\n",
      "GridSearchCV performed in : 15.0414431095\n",
      "\n",
      "r2 score is: 0.830549275096\n",
      "rmsle score is: 0.167670841697\n",
      "\n",
      "BaggingRegressor\n",
      "\n",
      "Training a BaggingRegressor using a training set size of 1164. . .\n",
      "BaggingRegressor Old regressor:\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=1, oob_score=False, random_state=2,\n",
      "         verbose=0, warm_start=False)\n",
      "\n",
      "Tuned regressor:\n",
      "BaggingRegressor(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=300, n_jobs=1, oob_score=False, random_state=2,\n",
      "         verbose=0, warm_start=False)\n",
      "\n",
      "Trained model in: 277.821322918\n",
      "Test model in : 0.584114074707\n",
      "GridSearchCV performed in : 277.821445942\n",
      "\n",
      "r2 score is: 0.85773202163\n",
      "rmsle score is: 0.153634732663\n",
      "\n",
      "AdaBoostRegressor\n",
      "\n",
      "Training a AdaBoostRegressor using a training set size of 1164. . .\n",
      "AdaBoostRegressor Old regressor:\n",
      "AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "         n_estimators=50, random_state=2)\n",
      "\n",
      "Tuned regressor:\n",
      "AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
      "         n_estimators=50, random_state=2)\n",
      "\n",
      "Trained model in: 25.3298699856\n",
      "Test model in : 0.0153090953827\n",
      "GridSearchCV performed in : 25.3300039768\n",
      "\n",
      "r2 score is: 0.781591924925\n",
      "rmsle score is: 0.190357579801\n",
      "\n",
      "XGBRegressor\n",
      "\n",
      "Training a XGBRegressor using a training set size of 1164. . .\n",
      "XGBRegressor Old regressor:\n",
      "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n",
      "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=nan, n_estimators=100, nthread=-1,\n",
      "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "\n",
      "Tuned regressor:\n",
      "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1.0,\n",
      "       gamma=0.0, learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
      "       min_child_weight=1, missing=nan, n_estimators=100, nthread=-1,\n",
      "       objective='reg:linear', reg_alpha=0.05, reg_lambda=0.6,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=0.6)\n",
      "\n",
      "Trained model in: 2131.24140406\n",
      "Test model in : 0.0338699817657\n",
      "GridSearchCV performed in : 2131.2416048\n",
      "\n",
      "r2 score is: 0.885504628192\n",
      "rmsle score is: 0.137825569226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tune_parameters(regressor_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save the regressors in the corresponding dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# top2 features with tuned regressors\n",
    "# regs_dict['top_2']['tuned'] = tuned_regressors\n",
    "\n",
    "# top10 features with tuned regressors\n",
    "# regs_dict['top_10']['tuned'] = tuned_regressors\n",
    "\n",
    "# all features with tuned regressors\n",
    "regs_dict['all']['tuned'] = tuned_regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the regressor dict via pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regs_dict.dict saved!\n"
     ]
    }
   ],
   "source": [
    "# Save regressors in dictionary\n",
    "filename = 'regs_dict.dict'\n",
    "pickle.dump(regs_dict, open(filename, 'wb'))\n",
    "print '{} saved!'.format(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
